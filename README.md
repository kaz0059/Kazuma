# ğŸ¤– AI Assistant

A sophisticated local AI assistant with smart response detection, knowledge base integration, and premium GUI interface.

## âœ¨ Features

- **ğŸ§  Smart Brain**: Automatically detects response style (length, tone, information type)
- **ğŸ’¾ Memory System**: Persistent conversation history with automatic backups  
- **ğŸ“š Knowledge Base**: RAG integration with PDF/TXT document support
- **ğŸ¨ Premium GUI**: Modern LobeChat-inspired web interface
- **ğŸ–¥ï¸ Console Mode**: Traditional command-line interface
- **ğŸ”’ Local**: All processing happens on your machine

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Ollama installed and running
- Llama3 model downloaded

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama
ollama serve

# Pull model
ollama pull llama3
```

### Installation

1. **Clone and setup environment:**
```bash
git clone <repository>
cd AI_Assistant
python -m venv renenv
renenv\Scripts\activate  # Windows
pip install -r requirements.txt
```

2. **Launch the assistant:**
```bash
# GUI version (recommended)
python gui_assistant.py

# Console version
python assistant.py

# Or use batch files (Windows)
start_gui.bat
start_console.bat
```

## ğŸ“ Project Structure

```
AI_Assistant/
â”œâ”€â”€ core/                    # Core modules
â”‚   â”œâ”€â”€ brain.py            # Smart AI brain
â”‚   â”œâ”€â”€ config.py           # Configuration management
â”‚   â”œâ”€â”€ memory.py           # Conversation memory
â”‚   â”œâ”€â”€ rag.py              # Knowledge base system
â”‚   â””â”€â”€ gui.py              # Modern web interface
â”œâ”€â”€ knowledge_base/         # Your documents (PDF/TXT)
â”œâ”€â”€ memory/                 # Conversation logs & backups
â”œâ”€â”€ assistant.py            # Console interface
â”œâ”€â”€ gui_assistant.py        # GUI launcher
â”œâ”€â”€ config.json            # Configuration file
â””â”€â”€ requirements.txt       # Dependencies
```

## ğŸ›ï¸ Configuration

Edit `config.json`:
```json
{
  "settings": {
    "memory_enabled": true
  },
  "api": {
    "provider": "ollama", 
    "model": "llama3"
  }
}
```

## ğŸ“š Knowledge Base

1. Add PDF or TXT files to `knowledge_base/` folder
2. Restart the assistant
3. Files are automatically indexed and used for context

## ğŸ’¡ Smart Features

The AI automatically detects what you want:

- **Length**: "briefly" â†’ short response, "explain in detail" â†’ comprehensive
- **Tone**: "hey" â†’ casual, "please" â†’ formal
- **Type**: "how to" â†’ instructions, "what is" â†’ definitions, "code" â†’ examples

## ğŸ”§ Development

```bash
# Activate environment
start_env.bat

# Run tests
python -m pytest tests/

# Update requirements
pip freeze > requirements.txt
```

## ğŸ“ Commands

### Console Mode
- `exit/quit` - Exit the assistant
- `clear` - Backup and clear conversation
- `stats` - Show memory statistics

### GUI Mode
- ğŸ—‘ï¸ Clear - Backup and clear chat
- ğŸ“¥ Export - Save conversation to markdown
- ğŸ“Š Stats - Show memory usage

## ğŸ› ï¸ Troubleshooting

### Common Issues

**"Ollama connection failed"**
```bash
# Check if Ollama is running
ollama list

# Start Ollama if not running  
ollama serve

# Verify model exists
ollama pull llama3
```

**"Module not found"**
```bash
# Reinstall dependencies
pip install -r requirements.txt
```

**"Port already in use"**
- GUI runs on port 7860 by default
- Check if another Gradio app is running

## ğŸ“Š Memory System

- Conversations stored in `memory/conversation_log.txt`
- Automatic backups in `memory/backups/`
- Supports both tuple and message formats
- Statistics tracking and export features

## ğŸš§ Future Enhancements

- [ ] Voice integration
- [ ] Multi-model support  
- [ ] Plugin system
- [ ] Mobile app
- [ ] Multi-user support
- [ ] Advanced document processing

## ğŸ“„ License

MIT License - feel free to modify and distribute.

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Make changes
4. Test thoroughly  
5. Submit pull request

---

**Enjoy your local AI assistant! ğŸ‰**